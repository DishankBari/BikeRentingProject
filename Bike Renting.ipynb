{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Problem Statement: </b> The objective of this Case is to Predication of bike rental count on daily based on the environmental and seasonal settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before starting this project with dataset. Initially I open given file in excel and just look the data available\n",
    "\n",
    "#starting the project we need to load some libraries to deal with this data\n",
    "\n",
    "import pandas as pd #for data processing & I/O operations\n",
    "import numpy as np #for mathematical calculations\n",
    "import seaborn as sns #for data visualization\n",
    "import matplotlib.pyplot as plt #for plotting graphs\n",
    "\n",
    "import sklearn #for machine learning algorithms\n",
    "\n",
    "import os #for setting directory, I/O file operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting working directories\n",
    "os.chdir(\"D:\\Bike Renting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the file directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load required dataset\n",
    "data = pd.read_csv(\"day.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the GIven Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after loading dataset\n",
    "#let's check the number of variables and observations in dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after performning above function We see that therer are <b>731 observations(Rows) and 16 variables(Columns)</b> in given dataset \n",
    "\n",
    "In this <b>cnt</b> is our Target Variable and the others are predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now explore dataset more\n",
    "#checking few dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checkingg few records of dataset, we get little bit confused about some of the variables values.\n",
    "\n",
    "But don't worry in our problem statement, we get clarification about these variables and their values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Important Note about Variables: </b>\n",
    "\n",
    "* instant: record index\n",
    "\n",
    "* dteday: date\n",
    "\n",
    "* season: season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "\n",
    "* yr: year (0: 2011, 1:2012)\n",
    "\n",
    "* mnth: month ( 1 to 12)\n",
    "\n",
    "* holiday: weather day is holiday or not (extracted from \n",
    "\n",
    "* weekday: day of the week\n",
    "\n",
    "* workingday: if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "\n",
    "* weathersit: (extracted fromFreemeteo) \n",
    "    * 1: Clear, Few clouds, Party cloudy, Partly cloudy\n",
    "    * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "\n",
    "* temp: Normalized temperature in Celsius. The values are divided via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale) \n",
    "\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are divided via (t-t_min)/(t_max- t_min), t_min=-16, t_max=+50 (only in hourly scale) \n",
    "\n",
    "* hum: Normalized humidity. The values are divided to 100 (max)\n",
    "\n",
    "* windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "\n",
    "* casual: count of casual users\n",
    "\n",
    "* registered: count of registered users\n",
    "\n",
    "* cnt: count of total rental bikes including both casual and registered\n",
    "\n",
    "\n",
    "this shows that most of the variables are already converted as Categorical Variables using Normalization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before moving further let's check the dtypes of each variables\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, most of the variables are categorical, and season, yr, mnth, holiday, weekday, workingday, weathersit variables should be a categorical type, but they are int64. Now we need to convert them into catergorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to categorical variables\n",
    "\n",
    "#so to convert muliple variables in one go, we need to create a loop function\n",
    "\n",
    "#create a variable and store all variables\n",
    "cat_var = [\"season\", \"yr\", \"mnth\", \"holiday\", \"weekday\", \"workingday\", \"weathersit\"]\n",
    "\n",
    "for i in cat_var:\n",
    "    data[i] = data[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking dtypes again\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, some of the varaibles are not useful for further analysis for that reason we are dropping some of the variables here:\n",
    "\n",
    "<b>Dropping Variables which are not required:</b>\n",
    "* instant - index number, which is not useful in analysis\n",
    "* dteday - all the required parameters are already extracted from this variable such as year, month, weekday. So this varaible is not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping instant and dteday variables\n",
    "data = data.drop(['instant', 'dteday'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the dataset after dropping two variables \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Analysis\n",
    "\n",
    "After converting dataset into proper format and dropping unuseful variables from dataset. Now its time to do Missing Value analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the missing values using isnull function\n",
    "\n",
    "data.isnull().sum().sort_values(ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing value present in given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check summary of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the <b>boxplot method</b> to visualize the outliers in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers in Target Variable \"cnt\" using boxplot method\n",
    "\n",
    "#plotting boxplot for cnt variable\n",
    "sns.boxplot(data =data, y=\"cnt\", orient ='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above boxplots, it is evident that there is no outliers present in cnt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's check the outliers of predictor variables such as temp, atemp, hum, windspeed, casual, registered\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols= 3)\n",
    "\n",
    "fig.set_size_inches(18,12)\n",
    "\n",
    "#plotting boxplot of temp variable\n",
    "sns.boxplot(data['temp'], orient ='v', ax=axes[0][0]).set_title(\"Boxplot of temp\")\n",
    "\n",
    "#plotting boxplot of atemp variable\n",
    "sns.boxplot(data['atemp'], orient ='v', ax=axes[0][1]).set_title(\"Boxplot of atemp\")\n",
    "\n",
    "#plotting boxplot of hum variable\n",
    "sns.boxplot(data['hum'], orient ='v', ax=axes[0][2]).set_title(\"Boxplot of hum\")\n",
    "\n",
    "#plotting boxplot of windspeed variable\n",
    "sns.boxplot(data['windspeed'], orient ='v', ax=axes[1][0]).set_title(\"Boxplot of windspeed\")\n",
    "\n",
    "#plotting boxplot of casual variable\n",
    "sns.boxplot(data['casual'], orient ='v', ax=axes[1][1]).set_title(\"Boxplot of casual\")\n",
    "\n",
    "#plotting boxplot of registered variable\n",
    "sns.boxplot(data['registered'], orient ='v', ax=axes[1][2]).set_title(\"Boxplot of registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as we see that there are some outliers value present in 'hum', 'windspeed', 'casual' variables.\n",
    "\n",
    "#before outlier removal lets findout the correlation analysis of these variables with target variables\n",
    "\n",
    "print(data['hum'].corr(data['cnt']))\n",
    "\n",
    "print(data['windspeed'].corr(data['cnt']))\n",
    "\n",
    "print(data['casual'].corr(data['cnt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols= 3)\n",
    "\n",
    "fig.set_size_inches(18,6)\n",
    "\n",
    "#Correlation between 'hum' and 'cnt'  before  removal of  outliers\n",
    "sns.regplot(x=\"hum\", y=\"cnt\", data=data, ax=ax1).set_title(\"Regression Plot of cnt vs hum\")\n",
    "\n",
    "#Correlation between 'windspeed' and 'cnt'  before  removal of  outliers\n",
    "sns.regplot(x=\"windspeed\", y=\"cnt\", data=data, ax=ax2).set_title(\"Regression Plot of cnt vs windspeed\")\n",
    "\n",
    "#Correlation between 'casual' and 'cnt'  before  removal of  outliers\n",
    "sns.regplot(x=\"casual\", y=\"cnt\", data=data, ax=ax3).set_title(\"Regression Plot of cnt vs casual\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Outliers:</b> As we see from boxplot, correlation and regression plot, varaibles <b>hum, windspeed, casual</b> has ouliers and that have to be remove by outlier removal method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make copy of dataset\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect & Delete Outliers from the dataset\n",
    "\n",
    "cnames = ['casual','hum','windspeed']\n",
    "\n",
    "for i in cnames:\n",
    "    q75, q25 = np.percentile(data.loc[:,i],[75,25])\n",
    "    iqr = q75 - q25\n",
    "    min = q25-(iqr*1.5)\n",
    "    max = q75 +(iqr*1.5)\n",
    "    print(min)\n",
    "    print(max)\n",
    "    data = data.drop(data[data.loc[:,i]< min].index)\n",
    "    data = data.drop(data[data.loc[:,i]>max].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check shape of dataset\n",
    "data.shape #58 obseravtions are dropped in outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Correlation Analysis: </b> Here I am generating correlation matrix to understand how the each variable related with each other. In that I am plotting correlation matrix and generate plot using seabon library for better understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating correlation matrix\n",
    "corr = data.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting correlation matrix and heatmap using seaborn libraty\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "sns.heatmap(corr,mask=np.zeros_like(corr, dtype=np.bool),cmap = sns.diverging_palette(220,10,as_cmap=True),square =True, annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Correlation Analysis Result</b>\n",
    "* temp and atemp are highly correlated\n",
    "* temp and atemp have positive and strong correlation with cnt\n",
    "* hum and windspeed have negative and weak correlation with cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping atemp variable from a dataset\n",
    "data = data.drop(['atemp'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploratory Data Analysis\n",
    "\n",
    "In <b>Exploratory Data Analysis</b> we are going to find the how each predictor or variables related with Target Varaible:\n",
    "\n",
    "* relation between Numerical Variable 'temp', 'hum', 'windspeed' and target variable 'cnt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding relationship between Numerical Variable 'temp', 'hum', 'windspeed' with target variable 'cnt'\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols= 3)\n",
    "\n",
    "fig.set_size_inches(15,9)\n",
    "\n",
    "#relation between 'temp' and 'cnt'\n",
    "sns.regplot(x=\"temp\", y=\"cnt\", data=data, ax=ax1).set_title(\"Regression Plot of cnt vs temp\")\n",
    "\n",
    "#relation between 'hum' and 'cnt'  \n",
    "sns.regplot(x=\"hum\", y=\"cnt\", data=data, ax=ax2).set_title(\"Regression Plot of cnt vs hum\")\n",
    "\n",
    "#relation between 'windspeed' and 'cnt'  \n",
    "sns.regplot(x=\"windspeed\", y=\"cnt\", data=data, ax=ax3).set_title(\"Regression Plot of cnt vs windspeed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we see that \n",
    "* cnt has positive linear relationship with temp, \n",
    "* on the other side, cnt has a negative linear relationship with windspeed. \n",
    "* But hum(Humidity) has a little negative linear relationship with cnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we find the relationship between categorical variables and Target Variable 'cnt'\n",
    "\n",
    "# categorical variables are \"Season\", \"holiday\", \"Weekday\", \"Workingday\", \"Weathersit\", \"month\"\n",
    "fig, axes = plt.subplots(nrows = 3, ncols=2)\n",
    "\n",
    "fig.set_size_inches(12,18)\n",
    "\n",
    "#plotting boxplot for cnt vs season variables\n",
    "sns.boxplot(data =data, y=\"cnt\", x=\"season\", orient='v', ax=axes[0][0]).set_title(\"Boxplot of cnt vs season\")\n",
    "\n",
    "#plotting boxplot for cnt vs holiday variables\n",
    "sns.boxplot(data =data, y=\"cnt\", x=\"holiday\", orient='v', ax=axes[0][1]).set_title(\"Boxplot of cnt vs holiday\")\n",
    "\n",
    "#plotting boxplot for cnt vs weekday variables\n",
    "sns.boxplot(data =data, y=\"cnt\", x=\"weekday\", orient ='v', ax=axes[1][0]).set_title(\"Boxplot of cnt vs weekday\")\n",
    "\n",
    "#plotting boxplot for cnt vs workingday variables\n",
    "sns.boxplot(data=data, y=\"cnt\", x=\"workingday\", orient='v', ax=axes[1][1]).set_title(\"Boxplot of cnt vs workingday\")\n",
    "\n",
    "#plotting boxplot for cnt vs month variables\n",
    "sns.boxplot(data=data, y=\"cnt\", x=\"mnth\", orient='v', ax=axes[2][0]).set_title(\"Boxplot of cnt vs mnth\")\n",
    "\n",
    "#plotting boxplot for cnt vs Weathershit variables\n",
    "sns.boxplot(data=data, y=\"cnt\", x=\"weathersit\", orient='v', ax=axes[2][1]).set_title(\"Boxplot of cnt vs weathersit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Results:</b>\n",
    "\n",
    "* <b>Graph 1:cnt vs season</b>:     \n",
    "    * cnt is very low in Spring Season and cnt is large in Fall Season \n",
    " \n",
    " \n",
    "* <b>Graph 2: cnt vs holiday</b> \n",
    "    * cnt is more on weekday i.e. no holiday\n",
    "    \n",
    "    \n",
    "* <b> Gradph 3: cnt vs weekday</b>\n",
    "    * as per the graph more number of bikes are used on Friday\n",
    "    \n",
    "    \n",
    "* <b> Gradph 4: cnt vs workingday</b>\n",
    "    * as per the graph more number of bikes are used on Workingday, this conclusion we already get from Graph 2\n",
    "    \n",
    "    \n",
    "* <b> Gradph 5: cnt vs month</b>\n",
    "    * as per the graph more number of bikes are used in July Month of year\n",
    "    \n",
    "    \n",
    "* <b> Gradph 6: cnt vs Weathersit</b>\n",
    "    * as per the graph more number of bikes are used when weather condition is Clear, Few clouds, Party cloudy, Partly cloudy\n",
    "    * and less number of bikes are used when weather condition is Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    * No bikes where used when weather condition is Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "    \n",
    "    \n",
    "    \n",
    "<b>Summary:</b>\n",
    "* cnt is maximum in good weather condition and minimum in bad weather condition\n",
    "* more number of bikes are rented on weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scalling \n",
    "\n",
    "as we know that, most of given variables are already Normalised\n",
    "\n",
    "But let's clarify that using Visualization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 2, ncols = 3)\n",
    "\n",
    "fig.set_size_inches(15,12)\n",
    "\n",
    "#Check whether  variable 'temp'is normal or not\n",
    "sns.distplot(data['temp'], ax=axes[0][0]).set_title(\"Normalization of temp\")\n",
    "\n",
    "#Check whether  variable 'hum'is normal or not\n",
    "sns.distplot(data['hum'], ax=axes[0][1]).set_title(\"Normalization of hum\")\n",
    "\n",
    "#Check whether  variable 'windspeed'is normal or not\n",
    "sns.distplot(data['windspeed'], ax=axes[0][2]).set_title(\"Normalization of windspeed\")\n",
    "\n",
    "#Check whether  variable 'casual'is normal or not\n",
    "sns.distplot(data['casual'],ax=axes[1][0]).set_title(\"Normalization of casual\")\n",
    "\n",
    "#Check whether  variable 'registered'is normal or not\n",
    "sns.distplot(data['registered'], ax=axes[1][1]).set_title(\"Normalization of registered\")\n",
    "\n",
    "#as well as Check whether  Target variable 'cnt'is normal or not\n",
    "sns.distplot(data['cnt'], ax=axes[1][2]).set_title(\"Normalization of Target Variable cnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our data is in proper scalling form. Numerical Predictor Variables are Normalized\n",
    "\n",
    "Also our Target Variable 'cnt' is also close to Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling & Prediction\n",
    "\n",
    "Data Cleaning is done! Now lets bulid the model and predict the results\n",
    "\n",
    "In machine Learning there is Two main types:\n",
    "* <b>Supervised Machine Learning :</b> knowledge of output. Target Variable is fix \n",
    "* <b>Unsupervised Machine Learning:</b> No knowledge of Output. Self Guided Learnig Algorithms.\n",
    "\n",
    "Selecting model is main Part of Modelling, We have various model algorithms some of the basic algorithms are:\n",
    "* <b>Linear Regression :</b> Best suitable for Regression Model\n",
    "* <b>Logistic Regression:</b> Suitable for Classification Model\n",
    "\n",
    "* <b>Decision Tree:</b> Best suitable for Regression & Classification model\n",
    "* <b>Random Forest:</b> Mostly used for Classification model analysis but can be use for Regression model\n",
    "\n",
    "* <b>KNN algorithms: </b> Can be used for Regression and Classification model\n",
    "* <b> Naive Bayes: </b> used for Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our given dataset, the Target Variable 'cnt' is Numerical Continuous Variable. So we are dealing with <b>Regression Model</b> Analysis.\n",
    "\n",
    "for that reason, we are considering following Algorithms:\n",
    "* Linear Regression\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* KNN Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before moving further \n",
    "#let's droped the casual and registered variables because there sum is equal to target variable ie. 'cnt'\n",
    "\n",
    "data = data.drop(['casual', 'registered'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's define the feature matrix and response vector\n",
    "X = data.drop('cnt', axis=1)\n",
    "y = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting X and y into training and testing dataset\n",
    "#import sklearn train_test_split library \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y =train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model using traing dataset\n",
    "#import LinearRegression libraries from sklearn\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model using training dataset\n",
    "model_LR = sm.OLS(train_y, train_X.astype(float)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things, we learn from this output\n",
    "\n",
    "* season, yr, weekday, workingday, weathersit, temp have small p-values, where as mnth, holiday, hum, windspeed have a larger p-values\n",
    "\n",
    "\n",
    "* Here we reject the null-hypothesis for season, yr, weekday, workingday, weathersit, temp\n",
    "    \n",
    "    \n",
    "    * There is assicoation between these variables and Target Variable cnt\n",
    "        \n",
    "        \n",
    "* Fail to reject the null hypothesis for mnth, holiday, hum, windspeed\n",
    "\n",
    "    * There is no association between these variables and Target Variable cnt\n",
    "    \n",
    "\n",
    "<b> R-squared (0.967)</b> means this model provides best fit for the given data \n",
    "\n",
    "but Selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "<b>Solution:</b>\n",
    "\n",
    " * <b>Adjusted R-squared</b>\n",
    " \n",
    " Penalizes model complexity (to control for overfitting), but it generally under-penalizes complexity.\n",
    " \n",
    "* <b>Better Solution:</b>\n",
    "\n",
    "Do model Evaluation based on the Error Metrics for Regression: \n",
    "\n",
    "For classification problems, we have only used classification accuracy as our evaluation metric. But here we used Error Metrics to evaluate the model\n",
    "\n",
    "<b>Mean Absolute Error (MAE):</b> is the mean of the absolute value of the errors: In [0,∞), the smaller the better\n",
    "\n",
    "\n",
    "<b>Mean Squared Error (MSE):</b> is the mean of the squared errors: In [0,∞), the smaller the better\n",
    "\n",
    "<b>Mean Absolute Percent Error (MAPE):</b> is the mean of the absolute percent value of the errors: In [0,1), the smaller the better\n",
    "\n",
    "\n",
    "<b>Root Mean Squared Error (RMSE) :</b>is the square root of the mean of the squared errors: In [0,∞), the smaller the better\n",
    "\n",
    "Let's calculate these by hand, to get an intuitive sense for the results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the predictions by model\n",
    "predict_LR = model_LR.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(true_y, pred_y):\n",
    "    mape = np.mean(np.abs(true_y-pred_y)/true_y)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important error metrics libraries\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# calculate MAE, MSE, MAPE, RMSE\n",
    "print(\"MAE:\",mean_absolute_error(test_y, predict_LR))\n",
    "print(\"MSE:\",mean_squared_error(test_y, predict_LR))\n",
    "print(\"MAPE:\",MAPE(test_y,predict_LR))\n",
    "print(\"RMSE:\",np.sqrt(mean_squared_error(test_y, predict_LR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MAE gives less weight to outliers means it is not sensitive to outliers.\n",
    "* MAPE is similar to MAE, but normalized the true obeservations. When true observation is zero then this metric will be problematic\n",
    "* MSE is a combination measurement of bias and variance of predictions. It is more popular.\n",
    "* RSME is square Root of MSE, Root square is taken to make the units of the error be the same as the units of the target. This measure gives more weight to large deviations such as outliers, since large differences squared become larger and small (smaller than 1) differences squared become smaller.\n",
    "\n",
    "<b>Selection:</b> Outoff these 4 error metrics, MSE and RMSE are mainly used for Time-Series dataset. As we know, current working data is not a time dependend or time-series data. \n",
    "\n",
    "for that Reason the Model Evaluation is based on <b>MAPE Error Metrics</b>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree regression\n",
    "#import DecisionTreeRegressor Analysis\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = DecisionTreeRegressor(max_depth = 2).fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the model on test data\n",
    "predict_DT = model_DT.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE, MSE, MAPE, RMSE\n",
    "print(\"MAE:\",mean_absolute_error(test_y, predict_DT))\n",
    "print(\"MSE:\",mean_squared_error(test_y, predict_DT))\n",
    "print(\"MAPE:\",MAPE(test_y,predict_DT))\n",
    "print(\"RMSE:\",np.sqrt(mean_squared_error(test_y, predict_DT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest analysis\n",
    "#imnport RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create Random Forest object\n",
    "model_RF = RandomForestRegressor(n_estimators = 50)\n",
    "\n",
    "##train the model using training dataset\n",
    "model_RF.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the predictions by model\n",
    "predict_RF = model_RF.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE, MSE, MAPE, RMSE\n",
    "print(\"MAE:\",mean_absolute_error(test_y, predict_RF))\n",
    "print(\"MSE:\",mean_squared_error(test_y, predict_RF))\n",
    "print(\"MAPE:\",MAPE(test_y,predict_RF))\n",
    "print(\"RMSE:\",np.sqrt(mean_squared_error(test_y, predict_RF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN implementation\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_KNN = KNeighborsRegressor(n_neighbors =5).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_KNN = model_KNN.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE, MSE, MAPE, RMSE\n",
    "print(\"MAE:\",mean_absolute_error(test_y, predict_KNN))\n",
    "print(\"MSE:\",mean_squared_error(test_y, predict_KNN))\n",
    "print(\"MAPE:\",MAPE(test_y,predict_KNN))\n",
    "print(\"RMSE:\",np.sqrt(mean_squared_error(test_y, predict_KNN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Best Fit model for Future Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are cosidering the MAPE for model evaluatiomn becasue, it calculate average absolute percent error for each time period minus actual values divided by actual values. \n",
    "\n",
    "Reason we already discussed, let's explain again:\n",
    "\n",
    "<b>Selection:</b> Outoff these 4 error metrics, MSE and RMSE are mainly used for Time-Series dataset. As I know, current working data is not a time dependend or time-series data.\n",
    "\n",
    "<b>Random Forest </b> Model has smallest error metrics i.e.\n",
    "* <b>MAPE = 0.1290541</b>\n",
    "\n",
    "So, for further analysis We are selecting <b>Random Forest Model.</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
